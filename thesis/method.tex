
\lhead[\chaptername~\thechapter]{\rightmark}

\rhead[\leftmark]{}

\lfoot[\thepage]{}

\cfoot{}

\rfoot[]{\thepage}

\chapter{Proposed Method}
\label{method}


\section{Hierarchical CNN}
\label{sec:HierCNN}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{thesis/images/hierarchical-cnn-fig.png}
    \caption{Regular CNN model on the left is converted to a hierarchical CNN. Users can use only some branches depending on their needs.}
    \label{fig:hiermethodoverview}
\end{figure}

Given a regular CNN model that can recognize many class categories, our method designs a hierarchical CNN which can be used partially according to each user's requirements. 
As it can be seen in the figure \ref{fig:hiermethodoverview}, users are able to use only a single branch of the hierarchical model, if they only require the categories of cats and dogs. 
In this way, our method is trying to achieve smaller, memory efficient CNN models that are adapted to each user's personal usages.

There are two steps in our method: creating a hierarchical representation of class categories and building CNN model according to the hierarchy.
In the following subsections, we explain the process of building the hierarchy and CNN model. 
Then, we share the details of how we generate realistic training and test users.
Lastly, we define the baselines to compare with our method.

\subsection{Method}



\subsection{Constructing the hierarchy}
\label{ssec:hierarchy}

According to our assumption, there is a high correlation between some pairs of classes in the sense that 
if users encounter a class, they are also likely to encounter another class if they are correlated. 
For example, in object recognition task, if users detect a computer, they are likely to detect a desk as well. 
By using this correlation, our objective is to place highly correlated classes closer in the hierarchy, 
so that users are most likely to find all the classes they need in nearby branches. 
As a result, users would use less branches and therefore less memory.

Given training user data that is described in subsection \ref{ssec:genusers}, we count the occurrences of class labels for each user. 
Secondly, we calculate the number of co-occurrences for every pair of class labels in the user data, obtaining a co-occurrence matrix.
Hierarchical distances between the class labels can be determined by inverting the ratios of the co-occurrence matrix, 
as the distance of two class labels is smaller when the number of co-occurences are larger. 
Applying hierarchical agglomerative clustering with the pre-calculated distances between each class label, we obtain a tree hierarchy as in the Figure \ref{fig:hierarchy}.

\subsection{Hierarchy to CNN model}
\label{ssec:hiertoCNN}

Obtaining the hierarchy as described in \ref{ssec:hierarchy}, we construct our hierarchical CNN model. Depending on the hyper-parameters, such as depth of the tree, branching positions etc., there are many ways to construct the structure of hierarchical CNN with reference to the obtained hierarchy.

The structure is designed as a tree-shaped CNN, where the input image enters through the root layer.
Each layer takes the result of its parent layer as input and forwards its output to the next layer. 
Following the final output of the leaf layer is the classification layer for the subset of classes corresponding to that specific leaf, obtained from the hierarchy.
Note that, an extra classification label called 'else' is added to each leaves to detect classes that do not belong to a specific branch.
In real-life scenario, if a user's model gives the output 'else', it means the encountered image has the class label that the user's model does not include.  
Therefore, additional model parts are downloaded until the image is recognized.

When constructing the tree-structured CNN, we followed some constraints as follows. 
In general, tree-structured CNN is designed by following a backbone network, such as VGG16 or MobileNet, as the base network, and branching it recursively to obtain a tree version of the network.
The structure is designed to be a binary tree, meaning a layer can only branch into two layers. 
Moreover, every path from the root to each leaf follows the same order of layers, matching with the layer order of the backbone network.
Finally, the size of the resulting tree-structured CNN and the corresponding backbone network must be comparable. 
The reason is that if users end up using the entire tree-structured CNN, they would require only as much storage as the backbone network.
To achieve this, after each and every branching, the number of convolutional channels in the following layers are reduced.

There are also hyper-parameters that affects the structure of the tree-based CNN. 
'Depth' hyper-parameter controls the number of times that the network can branch into two following networks. 
On the other hand, 'branching positions' changes where the branching happens.
For example, if the first position is '3', the network branches just before the 4th convolutional layer in the backbone network.

In the light of all the above-mentioned constraints and hyper-parameters, the hierarchical CNN is constructed as in the figure \ref{fig:hierarchy}. 
In this simple example, the backbone network consists of four subsequent convolutional layer modules. 
'Depth' is set as 3 and 'branching positions are set as 1,2,3. 
Note that, reduced number of convolutional channels are represented as the size of the layers in the figure.
As an implementation rule, we do not split the branch if the remaining class labels in the branch is 3 or less. 
That is why the left branch in the figure is not divided into further sub-branches.

\begin{figure}
    \centering
    \begin{minipage}[b]{.4\textwidth}
        \centering
        \includegraphics[width=.9\linewidth]{images/hierfig.png}
        %\caption{Hierarchical clustering of each class label in a 10-class example}
        \label{fig:sub1}
    \end{minipage}%
    \begin{minipage}[b]{.4\textwidth}
        \centering
        \includegraphics[width=.9\linewidth]{images/example_hier.png}
        %\caption{Hierarchical CNN obtained from the hierarchy on the left}
        \label{fig:sub2}
    \end{minipage}
    %\includegraphics[width=0.40\textwidth]{images/hierfig.png}
    \caption{Hierarchical clustering of each class label in a 10-class example and the resulting network structure}
    \label{fig:hierarchy}
\end{figure}

\subsection{Generating user types and case study users}

\label{ssec:genusers}

We require training and test users to create the hierarchy of our model and also test the final model in a scenario-based approach. 
Generated user data needs to be realistic as we chose to artificially generate the user data.

In our work, we assume that users interact with some classes significantly more than they interact with others due to their surroundings. 
Therefore, we defined a number of user types, each following a Dirichlet distribution, 
which the parameters of the distributions are selected to prioritize a subset of the classes. 
All users are generated according to one of the user type distribution. 
The user types can be interpreted as people with different surroundings, 
for example a chef is surrounded by kitchen tools whereas a pilot encounters airplanes on a daily basis.

When defining user types as Dirichlet distributions, we randomly put high values(200s) for high likely to encounter classes and low values(1s) for less likely to encounter classes. For example, a user type in 5-class classification would be $Dir(1,1,200,1,200)$. 
A test user generated from this specific distribution will encounter images in 3rd and 5th class much more than the rest of them. 
We chose the number of high values(200s) in a user type as 2 or 3 for testing CIFAR-10, which means each user is likely to encounter 2 or 3 classes out of 10 classes. 

To generate a user, a user type is chosen uniformly randomly between $m$ user types. 
Then, list of encountered class labels for the user are generated according to the chosen user type. 
An example of list of encountered class labels would be, $(3,3,5,3,5,5,...,2,...,3,5,3)$ if generated from the distribution $Dir(1,1,200,1,200)$. 
It can be interpreted as the user usually encounters 3rd and 5th classes and sometimes encounters other classes as well.
Finally, random images from the dataset are collected corresponding to the class categories in the list to make a scenario-based test set for the user.

On a final note, training users are generated to construct the hierarchy of the system, whereas test users are generated to test our model as a scenario-based approach. 
Both training and testing users are from the same user types as we assume correlation between the requirements of all users. 

\begin{figure}
    \centering
    \includegraphics[width=0.9\textwidth]{thesis/images/classification_baselines-fig.png}
    \caption{Left is the regular CNN model. Middle one is the proposed hierarchical CNN. Right is the multiple smaller regular CNNs. Note that, all models are comparable in size.}
    \label{fig:baselines}
\end{figure}

\subsection{Baselines}
\label{ssec:baselines}
The whole network size of the hierarchical CNN and the baseline networks are approximately equal, because it would be fair to compare the networks with similar number of parameters. 
We constructed the hierarchical CNN simply by following a backbone network's layer order and splitting into sub-branches according to the obtained hierarchy. 

Another straightforward approach to our problem is using multiple backbone networks each reduced in size with the task of classifying a subset of classes. 
These subsets of classes corresponds to the same subsets of classes recognized in the leaf nodes of hierarchical CNN.
Same as the hierarchical CNN, we reduce the size of the each network to make the total sum of the number of parameters of all networks equal to the backbone network. 
We will refer to this method as multiple CNNs. 

In this method, none of the networks share parameters, thus decreasing the size of each network more than that of hierarchical CNNs by also reducing the number of parameters in the earlier layers. 
However, reducing the number of extracted low-level features from the input image in the early layers would result in a decrease of accuracy. 
The decrease in the accuracy is dire especially when the number of sub-networks is higher because we reduce the size of the each network more to balance with the backbone network size. 
We compare both the methods and the backbone network in terms of their accuracy and average memory consumption per user. 
All three architecture can be seen in the figure \ref{fig:baselines}.

\begin{figure}
    \centering
    \includegraphics[width=0.9\textwidth]{thesis/images/image_ret_exp-fig.png}
    \caption{Overview of Image Retrieval}
    \label{fig:ret-exp}
\end{figure}


\section{Faster ANN}
\label{sec:FasterANN}

In image retrieval task, images are represented as vectors and the aim is to search a database of vectors for the nearest vector to the input image, as in figure \ref{fig:ret-exp}. 
For each query vector $\mathcal{Q} = \{\bm{q_1},...,\bm{q_M}\} \subset \mathbb{R}^D$ and database vectors $\mathcal{X} = \{\bm{x_1}, ..., \bm{x_N}\} \subset \mathbb{R}^D$, the task is to retrieve the most similar image with $n^* = \underset{n\in\{{1,...,N}\}}{\mathrm{argmin}} \vert\vert \bm{q} - \bm{x_n} \vert\vert $. 

Naive approach would be brute-force searching the entire database.
However, exhaustive searching of the database is slow and costly, especially with vectors with large dimensions. 
Thus, Approximate Nearest Neighbor (ANN) search algorithms are proposed with significantly faster search speed and a small probability of error. 
We built our work on the FAISS implementation~\cite{faiss} of Product Quantization~\cite{jegou2010product} with inverted multi-index as well as using it as our baseline.

As explained previously in section \ref{subsec:related-quantization}, product quantization method clusters the database space into $K$ clusters using k-means algorithm, $\mathcal{X}_1,...,\mathcal{X}_K$. 
We will refer to these clusters as subspaces or cells.
Then, the database vectors, $\bm{x_n}$, are assigned into one of $\mathcal{X}_k$.
When a query vector $\bm{q}\in\mathbb{R}^D$ is searched, first the nearest $\mathcal{X}_k$ is retrieved. 
Then, all vectors $\bm{x_n}$ in the $\mathcal{X}_k$ is traversed to find the nearest vector.

Our work aims to reduce the area of the search space by limiting the number of comparisons in the first step with $\mathcal{X}_k$ for each user according to their requirements. 
ANN search algorithms commonly assume that the distribution of the query vectors is the same with the distribution of the database vectors. 
However, real users have diverse query distributions, different than that of the database, 
so they query some parts of the search space more and the other parts less likely. 
This difference is sourcing from their limited surroundings in daily life or their different hobbies and interests. 
By utilizing this idea, we can reduce the search space by limiting the comparisons only to the relevant $\mathcal{X}_k$ for each user according to their requirements.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{thesis/images/improvement-fig.png}
    \caption{Query vectors and clustering of the search space is shown. Red crosses represent the position the query vectors fall in the space. We can choose to include every subspace with red crosses in our search as in the left figure. A hyper-parameter can control to include a subspace according to minimum occurrence of red crosses.}
    \label{fig:motivation}
\end{figure}

In figure \ref{fig:motivation}, query image examples of a user is shown with red crosses on the search space. 
For this user, a new query vector just needs to be compared with centroids in blue areas. 
We can reduce the amount of centroids to compare with a simple hyper-parameter, controlling whether to include a subspace according to minimum occurrence of the past query vectors. 
In this way, we can adjust the trade-off between accuracy and the speed of the system.

In the following subsections, our work will be explained in detail. 
Firstly, the details of using classification label information to reduce the area of search space will be explained. 
Next, the dataset and the distribution of class labels to subspaces will be discussed.
Then, the method of representing the images with vectors will be described in detail. 
After specifying the implementation details, baselines of our work will be defined.

\subsection{Using classification label information}

In our method, we used classification labels to represent the user requirements. 
Given the classification labels $\mathcal{C} = \{c_p\}_{p=1}^P$ where $P$ is the number different of labels, each user requirement is defined as $\mathcal{U}_i \subseteq \mathcal{C}$. 
Assuming we know the classification labels of our database vectors, they can be represented as $\mathcal{X} = \{(\bm{x_1}, c_{p1}), ..., (\bm{x_N}, c_{pN})\}$.
Then, the subspaces will also contain the class labels, $(\bm{x}, c_p) \in \mathcal{X}_k$.

Given a user requirement $\mathcal{U}_i$ to our system, we choose the corresponding subset of all subspaces according to the following algorithm.
If any of $(\bm{x}, c_p) \in \mathcal{X}_k$ contains the class label $c_p \in \mathcal{U}_i$, we consider that subspace as a relevant subspace to our user.
Let us define the relevant subspaces as $\mathcal{X}_\mathrm{rel} \subset \{\mathcal{X}_1,...,\mathcal{X}_K\}$.

In the search part of our algorithm, the query vector $\bm{q} \in \mathbb{R}^D$ is compared with the representative vectors of $\mathcal{X}_\mathrm{rel}$ instead of all the $\{\mathcal{X}_1,...,\mathcal{X}_K\}$. 
The remaining part of our algorithm is the same with the original PQ method.
Once the nearest $\mathcal{X}_k$ is found, the vectors associated with that subspace are traversed to find the nearest vector.

Our method accelerates the coarse quantization part of the PQ algorithm, where finding the closest subspace is the objective. 
We show the significance of accelerating the first part of PQ as the following.
Following the convention, we chose $K=\sqrt{N}$ where $N$ is the number of database vectors. 
In this way, number of vectors corresponding to $\mathcal{X}_k$ will be $\sqrt{N}$ in average. 
Once a query vector comes, the algorithm searches the closest centroid among $\sqrt{N}$ centroids. 
Then, the query vector is compared with the database vectors that belong to the chosen $\mathcal{X}_k$ and a few neighboring ones. 
In both steps, there are $O(\sqrt{N})$ comparisons to be done. 
Therefore, accelerating the first part is as important as accelerating the second part.

\subsection{Dataset and Distribution of Class Labels to Subspaces}
\label{retrievallabelinfo}

In our experiments, we used the scene recognition dataset called Places365\cite{zhou2017places}. 
There are 1.8 million training images with 365 class categories. 
1.3 million out of all images is used to construct our search database. The remaining images are used to train the ANN algorithm and also for the query vectors.
The class categories include various scenes such as airport terminal, library, train station, waterfall, etc. 
With such diverse scenes, users would interact with only a subset of the class categories and our idea can be easily applied to speed up the search according to each user's requirements.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{thesis/images/label_dist-fig.png}
    \caption{Distribution of the images according to their class labels in the first 20 subspaces}
    \label{fig:label-dist}
\end{figure}

Following the convention, we chose the number of subspaces as $K=\sqrt{N}$ where $N$ is the number of database vectors. 
Therefore, the search space is divided into 1140 subspaces which is the square root of the number of images($\sqrt{N}$) in our search database. 
Distribution of the images to those subspaces according to their classification labels can be seen in the figure \ref{fig:label-dist}.
As can be seen in the figure, some subspaces have extreme bias towards a few class, while others do not have much. 
There are many class categories in Places dataset that have similar images, such as "elevator-door", "elevator-interior", "elevator lobby", "elevator shaft", etc.
For example, the subspace with id:17 in the figure has labels with very similar images.


\subsection{Extracting Deep Features of Images}
\label{extractdeepfeatures}

\begin{figure}
    \centering
    \includegraphics[width=0.9\textwidth]{thesis/images/deep_features-fig.png}
    \caption{Last fully connected layer of VGG16\cite{simonyan2014very} is discarded to obtain a descriptor vector with dimension 4096.}
    \label{fig:vec-repr}
\end{figure}

In the work of Babenko \emph{et al.}\cite{babenko2014neural}, using a pre-trained convolutional neural network in ImageNet for image retrieval task is shown to have decent results. 
Note that, it is also shown that if the network is trained on the same data with the retrieval dataset, the results are better.
However, instead of getting the best performance, we simply investigate the comparison of our idea with the current best methods.
Therefore, we used VGG16\cite{simonyan2014very} network that is pre-trained on ImageNet\cite{deng2009imagenet}. 
The last fully connected layer is discarded to get a vector representation with 4096 dimensions, as in figure \ref{fig:vec-repr}.
Then, all database images are passed through the model once to get their descriptor vectors.

However, dimensions of the vectors were quite big for both storage and calculation. 
As stated in \cite{jegou2012negative}, applying whitening on the descriptor vectors not only helps with the computational and storage costs, but also improves the accuracy of retrieval tasks.
We applied Principal Component Analysis (PCA) to obtain the final descriptor vectors with 64 dimensions.


\subsection{Implementation details}

As mentioned before, we built our work on FAISS implementation\cite{faiss} of product quantization with inverted index, namely IndexIVFPQ. 
The code is shared publicly and we modified the code to tailor to our needs.
Extra function is added to adjust the search space according to a given list. 
This function is called during the indexing phase of the database before the search.

The training phase is required for IndexIVFPQ and it needs different vectors than the database vectors with similar distribution.
Training dataset of Places365 contains 1.8 million images. 
After obtaining the descriptor vectors of the images, we split the vectors into three parts.
Base vectors containing roughly 1.3 million vectors to form the search space, about a half million training vectors to train IndexIVFPQ and remaining approximately 3000 for query vectors.
The vectors are split into 3 parts in a way that the number of images for each label in all parts are almost equal.
When querying for a user, only the images with the required labels out of 3000 are used for the query images.

\subsection{Baselines}

Our main baseline is the FAISS implementation\cite{faiss} of product quantization with inverted index, namely IndexIVFPQ.
We compared our method with IndexIVFPQ by evaluating both method for search time per query and recall at 1 where recall at $k$ checks if the true result is one of the $k$ predicted results. 

